defaults:
  - dset: debug
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

# model related
model: phonetic_aware_demucs # either demucs or dwave
hidden: 48
stride: 4
resample: 4

demucs:
  chin: 1
  chout: 1
  hidden: ${hidden}
  max_hidden: 10000
  causal: False
  glu: true
  depth: 5
  kernel_size: 8
  stride: ${stride}
  normalize: true
  resample: ${resample}
  growth: 2
  rescale: 0.1

features_dim: 768   # 512 for ASR, 768 for hubert
features_dim_for_conditioning: 768   # 768 for hidden 48, 1024 for hidden 64
include_ft: False
get_ft_after_lstm: False
use_as_conditioning: True
use_as_supervision: False
layers: [6]
#layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
learnable: True

features_config:
  include_ft: ${include_ft}
  feature_model: 'hubert'
  state_dict_path: '/cs/labs/adiyoss/shared/pretrained_models/hubert/hubert_base_ls960.pt'
  features_factor: 0.01
  features_dim: ${features_dim}
  get_ft_after_lstm: ${get_ft_after_lstm}
  use_as_conditioning: ${use_as_conditioning}
  merge_method: 'inter'
  features_dim_for_conditioning: ${features_dim_for_conditioning}
  use_as_supervision: ${use_as_supervision}
  supervision_factor: 0.1
  layers: ${layers}
  learnable: ${learnable}
  device: ${device}

# Dataset related - for DsetBuilder
dset_builder:  noisy_clean

# Experiment name - this will define the results dir name to which experiment would be saved to
experiment_name: h48u4s4_hubert_conditioning_pre_lstm_debug_new

include_pretraining: False

# Logging and printing, and does not impact training
num_prints: 5
device: cuda
num_workers: 2
verbose: 0
show: 0   # just show the model and its size and exit

# Checkpointing, by default automatically load last checkpoint
checkpoint: true
continue_from: '' # Path the a checkpoint.th file to start from.
                  # this is not used in the name of the experiment!
                  # so use a dummy=something not to mixup experiments.
continue_best: false  # continue from best, not last state if continue_from is set.
continue_pretrained:   # use either dns48, dns64 or master64 to fine tune from pretrained-model
restart: false # Ignore existing checkpoints
checkpoint_file: checkpoint.th
best_file: best.th  # will contain only best model at any point
history_file: history.json
samples_dir: samples
save_again: false  # if true, only load checkpoint and save again, useful to reexport best.th

# Other stuff
seed: 2036
dummy:  # use this if you want twice the same exp, with a different name

# Evaluation stuff
eval_every: 10  # compute test metrics every so epochs
dry: 0.  # dry/wet knob value at eval
do_enhance: False

# Optimization related
optim: adam
lr: 3e-4
beta: 0.9
beta2: 0.999
epochs: 300
batch_size: 16
loss: l1
stft_loss: True
stft_sc_factor: 0.1
stft_mag_factor: 0.1

# Solvers
solver_name: phonetic_aware_solver

# Models
model_name: phonetic_aware_demucs

# Weights and Biases
wandb_mode: online
wandb:
  mode: ${wandb_mode} # online/offline/disabled;
  log: all # gradients/parameters/all/None
  log_freq: 5
  n_files_to_log: 5 # number or -1 for all files
  wandb_entity: 'huji-dl-audio-lab'
  project: "Lexical Aware Speech Denoising"

# log results
log_results: true
n_bins: 10

# Experiment launching, distributed
ddp: False
ddp_backend: nccl
rendezvous_file: ./rendezvous

# Internal config, don't set manually
rank:
world_size:


# Hydra config
hydra:
  run:
    dir: ./outputs/${experiment_name}  # this specifies where the model outputs will be saved to
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know...
        exclude_keys: [
          'hydra.job_logging.handles.file.filename',
          'dset.train', 'dset.valid', 'dset.test', 'dset.noisy_json', 'dset.noisy_dir',
          'num_prints', 'continue_from', 'save_again',
          'device', 'num_workers', 'print_freq', 'restart', 'verbose',
          'log', 'ddp', 'ddp_backend', 'rendezvous_file', 'rank', 'world_size']
  job_logging:
    handlers:
      file:
        class: logging.FileHandler
        mode: w
        formatter: colorlog
        filename: trainer.log
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr
